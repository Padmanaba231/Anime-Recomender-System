# -*- coding: utf-8 -*-
"""Salinan_dari_Untitled0_(2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10l83r_oJE7G4--eWGNf0bINwqyGrxKBK

#__Data Understanding__
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib
import matplotlib.pyplot as plt
# %matplotlib inline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity
import zipfile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

"""####Instal package kaggle"""

! pip install -q kaggle

"""####Upload Json Profile Kaggle"""

from google.colab import files

files.upload()

"""####Membuat direktori dan mengatur izin agar bisa memasukan JSON Kaggle"""

! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

"""####Mendownload Dataset"""

!kaggle datasets download -d CooperUnion/anime-recommendations-database

"""####Ekstrak Zipfile"""

local_zip = '/content/anime-recommendations-database.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/content')
zip_ref.close()

"""####Memuat dataset ke dalam Pandas Dataframe"""

df = pd.read_csv('anime.csv')
df.head()

rating = pd.read_csv('rating.csv')
rating.head()

"""####Mengecek informasi dari dataset"""

df.shape

rating.shape

df.info()

rating.info()

"""##__Exploratory Data Analysis__

####Mengecek missing value
"""

df.isnull().sum()

#untuk rating, pada deskripsi fitur rating dikatakan rating -1 menandakan user tidak memberi rating pada anime tersebut, hal ini termasuk ke dalam missing value
(rating['rating'] == -1).sum()

"""###EDA Dataframe df

####Mengecek fitur type
"""

count = df[['type']].value_counts()
count.plot(kind='bar', title=['type'], subplots=True)

"""####Mengecek persebaran fitur genre"""

df['genre'].unique()

"""Bisa dilihat dari output diatas, fitur genre memiliki nilai unik pada setiap data/baris

####Mengecek fitur rating
"""

sns.boxplot(data=df, x=df['rating'])

df.describe()

"""Bisa dilihat dari output di atas, fitur rating tidak memiliki outlier. Hal ini karena rentang nilai sesuai dengan deskripsi fitur dimana fitur rating memiliki nilai rentang dari 0 hingga 10

###EDA Dataframe rating
"""

countr = rating[['rating']].value_counts()
countr.plot(kind='bar', title=['rating'], subplots=True)

"""#__Data Preparation__

####Mengatasi missing value
"""

df = df.dropna()
df.isnull().sum()

rating = rating[rating['rating'] != -1]
(rating['rating'] == -1).sum()

"""####Reduksi fitur

Beberapa fitur yang tidak relevant akan dieliminasi. Pada kasus ini fitur episodes dan members tidak diperlukan.
"""

df.drop(['episodes','members','anime_id'],inplace=True,axis=1)
df.head()

"""####Mengecek jumlah user dan anime pada dataframe rating setelah missing value dikeluarkan"""

jum_user = rating['user_id'].nunique()
jum_anime = rating['anime_id'].nunique()
min_rating = min(rating['rating'])
max_rating = max(rating['rating'])

print(f"Jumlah user': {jum_user}")
print(f"Jumlah anime': {jum_anime}")
print(f"Minimum rating': {min_rating}")
print(f"Maximum Rating': {max_rating}")

"""####Encode user_id dan anime_id"""

user_ids = rating['user_id'].unique().tolist()
print('list user_id: ', user_ids)

user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded user_id : ', user_to_user_encoded)

user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke user_id: ', user_encoded_to_user)

anime_ids = rating['anime_id'].unique().tolist()

anime_to_anime_encoded = {x: i for i, x in enumerate(anime_ids)}

anime_encoded_to_anime = {i: x for i, x in enumerate(anime_ids)}

"""####Memetakan kedalam dataframe"""

rating['user'] = rating['user_id'].map(user_to_user_encoded)

rating['anime'] = rating['anime_id'].map(anime_to_anime_encoded)

"""####Mengecek jumlah user dan anime setelah encoding"""

num_users = len(user_to_user_encoded)
print(num_users)

num_anime = len(anime_encoded_to_anime)
print(num_anime)

rating['rating'] = rating['rating'].values.astype(np.float32)

min_rating = min(rating['rating'])

max_rating = max(rating['rating'])

print('Number of User: {}, Number of anime: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_anime, min_rating, max_rating
))

"""#### membuat dictionary untuk menentukan pasangan key-value pada data anime_id, anime_name, dan anime_genre"""

df_sementara = pd.read_csv("anime.csv")

anime_id = df_sementara['anime_id'].tolist()
anime_name = df_sementara['name'].tolist()
anime_genre = df_sementara['genre'].tolist()

print(len(anime_id))
print(len(anime_name))
print(len(anime_genre))

anime_new = pd.DataFrame({
    'id': anime_id,
    'anime_name': anime_name,
    'genre': anime_genre
})
anime_new

"""####Membagi data menjadi data latih dan data validasi"""

#mengacak dataframe agar distribusinya menjadi random
rating = rating.sample(frac=1, random_state=42)
rating

x = rating[['user_id', 'anime_id']].values
y = rating['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * rating.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""#__Modeling and Result__

##__A. Content Based Filtering__

Pada bagian Content Based Filtering, model yang digunakan adalah model Cosine Similarity. Pada proyek ini akan dicoba menggunakan 2 filter, yaitu Model Content Based Filtering berdasarkan fitur genre, Content Based Filtering berdasarkan fitur type

###a. Content Based Filtering (genre)

####Mengekstrak fitur genre dengan TF-IDF Vectorizer
"""

# Fungsi kustom untuk memisahkan teks berdasarkan koma
def custom_tokenizer(text):
    return text.split(', ')

# Inisialisasi TfidfVectorizer dengan tokenizer kustom
tfidf = TfidfVectorizer(tokenizer=custom_tokenizer)

# Hitung idf pada kategori yang dipilih
tfidf.fit(df['genre'])

# Mapping array dari fitur index integer ke fitur nama
tfidf.get_feature_names_out()

"""####transformasi ke dalam matriks"""

tfidf_matrix = tfidf.fit_transform(df['genre'])
tfidf_matrix.shape

tfidf_matrix.todense()

"""####Melihat hasil matriks tf-idf"""

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tfidf.get_feature_names_out(),
    index=df.name
).sample(22, axis=1).sample(10, axis=0)

"""Output di atas akan menampilkan hubungan antara nama anime dengan genre yang ada

####Cosine Similarity
"""

cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""#####Melihat hasil matriks cosine similarity"""

cosine_sim_df = pd.DataFrame(cosine_sim, index=df['name'], columns=df['name'])
print('Shape:', cosine_sim_df.shape)
cosine_sim_df.sample(5, axis=1).sample(5, axis=0)

"""Output di atas menampilkan hubungan antara satu anime dengan anime lainnya

####Top-N Recomendation

#####Membuat fungsi Top-N Recomendation
"""

def anime_recommendations(anime_name, similarity_data=cosine_sim_df, items=df[['name', 'genre']], k=5):

    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,anime_name].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop anime_name agar nama anime yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(anime_name, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

"""#####Menemukan Rekomendasi anime"""

df[df.name.eq('Charlotte')]

anime_recommendations('Charlotte')

"""###b. Content Based Filtering (type)

####Mengekstrak fitur genre dengan TF-IDF Vectorizer
"""

# Fungsi kustom untuk memisahkan teks berdasarkan koma
def custom_tokenizer(text):
    return text.split(', ')

# Inisialisasi TfidfVectorizer dengan tokenizer kustom
tfidf = TfidfVectorizer(tokenizer=custom_tokenizer)

# Hitung idf pada kategori yang dipilih
tfidf.fit(df['type'])

# Mapping array dari fitur index integer ke fitur nama
tfidf.get_feature_names_out()

"""####transformasi ke dalam matriks"""

tfidf_matrix = tfidf.fit_transform(df['type'])
tfidf_matrix.shape

tfidf_matrix.todense()

"""####Melihat hasil matriks tf-idf"""

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tfidf.get_feature_names_out(),
    index=df.name
).sample(5, axis=1).sample(10, axis=0)

"""####Cosine Similarity"""

cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""#####Melihat hasil matriks cosine similarity"""

cosine_sim_df = pd.DataFrame(cosine_sim, index=df['name'], columns=df['name'])
print('Shape:', cosine_sim_df.shape)
cosine_sim_df.sample(5, axis=1).sample(5, axis=0)

"""####Top-N Recomendation"""

def anime_recommendations2(anime_name, similarity_data=cosine_sim_df, items=df[['name', 'type']], k=5):

    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,anime_name].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop anime_name agar nama anime yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(anime_name, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

"""#####Menemukan Rekomendasi anime"""

df[df.name.eq('Charlotte')]

anime_recommendations2('Charlotte')

"""##__B. Collaborative Filtering__

###Training Model
"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, jum_user, jum_anime, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.jum_user = jum_user
    self.jum_anime = jum_anime
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        jum_user,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(jum_user, 1) # layer embedding user bias
    self.anime_embedding = layers.Embedding( # layer embeddings anime
        jum_anime,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.anime_bias = layers.Embedding(jum_anime, 1) # layer embedding anime bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    anime_vector = self.anime_embedding(inputs[:, 1]) # memanggil layer embedding 3
    anime_bias = self.anime_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_anime = tf.tensordot(user_vector, anime_vector, 2)

    x = dot_user_anime + user_bias + anime_bias

    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(jum_user, jum_anime, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

from tensorflow.keras.callbacks import EarlyStopping

def create_early_stopping():

    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=3,
        restore_best_weights=True,
        verbose=1
    )

    return early_stopping

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 32,
    epochs = 20,
    verbose=1,
    validation_data = (x_val, y_val),
    callbacks=[create_early_stopping()]
)

anime_df = anime_new
df_pre = pd.read_csv('rating.csv')

# Mengambil sample user
user_id = df_pre.user_id.sample(1).iloc[0]
anime_visited_by_user = df_pre[df_pre.user_id == user_id]

# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html
anime_not_visited = anime_df[~anime_df['id'].isin(anime_visited_by_user.anime_id.values)]['id']
anime_not_visited = list(
    set(anime_not_visited)
    .intersection(set(anime_to_anime_encoded.keys()))
)

anime_not_visited = [[anime_to_anime_encoded.get(x)] for x in anime_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_anime_array = np.hstack(
    ([[user_encoder]] * len(anime_not_visited), anime_not_visited)
)

ratings = model.predict(user_anime_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_anime_ids = [
    anime_encoded_to_anime.get(anime_not_visited[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('anime with high ratings from user')
print('----' * 8)

top_anime_user = (
    anime_visited_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .anime_id.values
)

anime_df_rows = anime_df[anime_df['id'].isin(top_anime_user)]
for row in anime_df_rows.itertuples():
    print(row.anime_name, ':', row.genre)

print('----' * 8)
print('Top 10 anime recommendation')
print('----' * 8)

recommended_anime = anime_df[anime_df['id'].isin(recommended_anime_ids)]
for row in recommended_anime.itertuples():
    print(row.anime_name, ':', row.genre)

"""#Evaluation

###Precision

Precision digunakan untuk mengukur kinerja sistem. Precision adalah tingkat ketepatan antara informasi yang diminta oleh pengguna dengan jawaban yang diberikan oleh sistem
<br><br>
Berdasarkan hasil ujicoba pada model Content Base Filtering menggunakan Filter Genre, dan Type didapat bahwa 5/5 hasil rekomendasi memiliki jenis Genre, dan Type yang serupa dengan anime yang sebelumnya ditonton oleh pengguna. Dengan demikian Top-5 Reccomender System yang dibangun memiliki presisi sebesar 5/5 = 100%

###RMSE
"""

plt.plot(history.history['root_mean_squared_error'])

plt.plot(history.history['val_root_mean_squared_error'])

plt.title('model_metrics')

plt.ylabel('root_mean_squared_error')

plt.xlabel('epoch')

plt.legend(['train', 'test'], loc='upper left')

plt.show()